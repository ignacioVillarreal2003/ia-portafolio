{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhbj_vbJHqS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a2c280-493c-4520-a810-37d859817bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics: 8.3.222\n",
            "CUDA disponible: True\n",
            "GPU: Tesla T4\n",
            "Device: 0\n"
          ]
        }
      ],
      "source": [
        "# UT3-11 (Parcial): Comparativa YOLO y Tracking\n",
        "# Cobertura: Trabajo 1 (parcial), Trabajo 2 (parcial), Trabajo 3 (parcial)\n",
        "# Objetivo: rapidez para experimentar, no entrenar todo\n",
        "\n",
        "# === InstalaciÃ³n rÃ¡pida ===\n",
        "!pip install -q ultralytics opencv-python matplotlib numpy pandas seaborn gdown norfair motmetrics\n",
        "\n",
        "# Detectron2 (opcional, puede requerir CUDA/Colab compatible)\n",
        "# Si falla en local, deja esta celda comentada o ejecÃºtala en Colab con CUDA 11.8\n",
        "# !pip install detectron2 -f \\\n",
        "#   https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html\n",
        "\n",
        "import os, time, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import ultralytics\n",
        "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Norfair\n",
        "from norfair import Detection, Tracker\n",
        "from norfair.distances import mean_euclidean\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "device_choosen = 0 if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device_choosen}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup de Dataset (autosuficiente) ===\n",
        "# Descarga desde Kaggle: lakshaytyagi01/fruit-detection (formato YOLO)\n",
        "\n",
        "import os, glob, yaml, shutil, subprocess, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Instalar Kaggle CLI si no estÃ¡\n",
        "try:\n",
        "    import kaggle  # type: ignore\n",
        "except Exception:\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"kaggle\"], check=False)\n",
        "\n",
        "# Asegurar kaggle.json en ~/.kaggle (si existe localmente, copiarlo)\n",
        "home_kaggle = Path.home() / \".kaggle\"\n",
        "home_kaggle.mkdir(parents=True, exist_ok=True)\n",
        "if Path(\"kaggle.json\").exists() and not (home_kaggle / \"kaggle.json\").exists():\n",
        "    shutil.copyfile(\"kaggle.json\", home_kaggle / \"kaggle.json\")\n",
        "    try:\n",
        "        os.chmod(home_kaggle / \"kaggle.json\", 0o600)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"\\n=== DESCARGANDO DATASET DE KAGGLE ===\")\n",
        "DATASET_ZIP = Path('fruit-detection.zip')\n",
        "DATASET_DIR = Path('fruit_detection')\n",
        "\n",
        "if not DATASET_DIR.exists():\n",
        "    # Descargar con Kaggle CLI\n",
        "    subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \"lakshaytyagi01/fruit-detection\", \"-p\", \".\", \"-q\"], check=False)\n",
        "    if DATASET_ZIP.exists():\n",
        "        DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        with zipfile.ZipFile(DATASET_ZIP, 'r') as zf:\n",
        "            zf.extractall(DATASET_DIR)\n",
        "        print(\"âœ… Dataset descomprimido en:\", DATASET_DIR)\n",
        "    else:\n",
        "        print(\"âŒ No se pudo descargar el zip. Asegura kaggle.json en ~/.kaggle o en el directorio actual.\")\n",
        "else:\n",
        "    print(\"Dataset ya presente en:\", DATASET_DIR)\n",
        "\n",
        "# Verificar/crear data.yaml\n",
        "print(\"\\n=== VERIFICANDO/CREANDO data.yaml ===\")\n",
        "yaml_files = list(DATASET_DIR.glob('**/data.yaml'))\n",
        "if yaml_files:\n",
        "    YAML_PATH = yaml_files[0]\n",
        "else:\n",
        "    # Intentar detectar estructura comÃºn\n",
        "    train_dirs = list(DATASET_DIR.glob('**/train/images')) + list(DATASET_DIR.glob('**/images/train'))\n",
        "    val_dirs = list(DATASET_DIR.glob('**/valid/images')) + list(DATASET_DIR.glob('**/val/images')) + list(DATASET_DIR.glob('**/images/val'))\n",
        "\n",
        "    if train_dirs and val_dirs:\n",
        "        train_path = str(train_dirs[0].relative_to(DATASET_DIR))\n",
        "        val_path = str(val_dirs[0].relative_to(DATASET_DIR))\n",
        "    else:\n",
        "        train_path = 'train/images'\n",
        "        val_path = 'valid/images'\n",
        "\n",
        "    # Clases tÃ­picas del dataset\n",
        "    fruit_classes = ['apple', 'banana', 'grapes', 'orange', 'pineapple', 'watermelon']\n",
        "\n",
        "    data_config = {\n",
        "        'path': str(DATASET_DIR.absolute()),\n",
        "        'train': train_path,\n",
        "        'val': val_path,\n",
        "        'nc': len(fruit_classes),\n",
        "        'names': fruit_classes\n",
        "    }\n",
        "    YAML_PATH = DATASET_DIR / 'data.yaml'\n",
        "    with open(YAML_PATH, 'w') as f:\n",
        "        yaml.dump(data_config, f)\n",
        "    print(\"âœ… Creado:\", YAML_PATH)\n",
        "\n",
        "# EstadÃ­sticas rÃ¡pidas\n",
        "def count_images(rel_path: str) -> int:\n",
        "    return len(list((DATASET_DIR / rel_path).glob(\"*.jpg\"))) + len(list((DATASET_DIR / rel_path).glob(\"*.png\")))\n",
        "\n",
        "with open(YAML_PATH, 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "train_rel = cfg['train']\n",
        "val_rel = cfg['val']\n",
        "\n",
        "# Si las rutas son absolutas, intentar corregir a relativas bajo DATASET_DIR\n",
        "def find_split_dir(root: Path, split: str) -> str | None:\n",
        "    # Busca 'train/images' o 'valid/images' dentro del root\n",
        "    candidates = list(root.glob(f\"**/{split}/images\")) + list(root.glob(f\"**/images/{'train' if split=='train' else 'val'}\"))\n",
        "    if candidates:\n",
        "        try:\n",
        "            return str(candidates[0].relative_to(root))\n",
        "        except Exception:\n",
        "            return str(candidates[0])\n",
        "    return None\n",
        "\n",
        "def is_abs(p: str) -> bool:\n",
        "    try:\n",
        "        return Path(p).is_absolute()\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# Determinar nuevas rutas si son absolutas o no existen\n",
        "needs_fix = False\n",
        "new_train = train_rel\n",
        "new_val = val_rel\n",
        "if is_abs(train_rel) or not (DATASET_DIR / train_rel).exists():\n",
        "    cand = find_split_dir(DATASET_DIR, 'train')\n",
        "    if cand:\n",
        "        new_train = cand\n",
        "        needs_fix = True\n",
        "if is_abs(val_rel) or not (DATASET_DIR / val_rel).exists():\n",
        "    cand = find_split_dir(DATASET_DIR, 'valid')\n",
        "    if cand:\n",
        "        new_val = cand\n",
        "        needs_fix = True\n",
        "\n",
        "# Guardar data_fixed.yaml si fue necesario\n",
        "if needs_fix:\n",
        "    data_fixed = dict(cfg)\n",
        "    data_fixed['path'] = str(DATASET_DIR.absolute())\n",
        "    data_fixed['train'] = new_train\n",
        "    data_fixed['val'] = new_val\n",
        "    YAML_PATH = DATASET_DIR / 'data_fixed.yaml'\n",
        "    with open(YAML_PATH, 'w') as f:\n",
        "        yaml.dump(data_fixed, f)\n",
        "    print(\"ğŸ”§ Rutas absolutas detectadas. Creado data_fixed.yaml con rutas relativas:\")\n",
        "    print(\"   train:\", new_train)\n",
        "    print(\"   val:  \", new_val)\n",
        "else:\n",
        "    YAML_PATH = YAML_PATH  # mantener\n",
        "\n",
        "# Conteos robustos (acepta abs/rel)\n",
        "def count_images_any(p: str) -> int:\n",
        "    pth = Path(p)\n",
        "    if pth.is_absolute():\n",
        "        return len(list(pth.glob('*.jpg'))) + len(list(pth.glob('*.png')))\n",
        "    else:\n",
        "        return len(list((DATASET_DIR / p).glob('*.jpg'))) + len(list((DATASET_DIR / p).glob('*.png')))\n",
        "\n",
        "print(f\"\\nğŸ“ Train images dir: {new_train}\")\n",
        "print(f\"ğŸ“ Val images dir:   {new_val}\")\n",
        "print(f\"   Train imgs: {count_images_any(new_train)} | Val imgs: {count_images_any(new_val)}\")\n",
        "\n",
        "# Variables globales para el resto del notebook\n",
        "DATASET_ROOT = DATASET_DIR\n",
        "print(\"\\nâœ… Dataset listo. YAML_PATH =\", YAML_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v9-AMPcINH5",
        "outputId": "080159d4-7a82-4413-ae82-dbdcd3486138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DESCARGANDO DATASET DE KAGGLE ===\n",
            "Dataset ya presente en: fruit_detection\n",
            "\n",
            "=== VERIFICANDO/CREANDO data.yaml ===\n",
            "ğŸ”§ Rutas absolutas detectadas. Creado data_fixed.yaml con rutas relativas:\n",
            "   train: Fruits-detection/train/images\n",
            "   val:   Fruits-detection/valid/images\n",
            "\n",
            "ğŸ“ Train images dir: Fruits-detection/train/images\n",
            "ğŸ“ Val images dir:   Fruits-detection/valid/images\n",
            "   Train imgs: 7108 | Val imgs: 914\n",
            "\n",
            "âœ… Dataset listo. YAML_PATH = fruit_detection/data_fixed.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Trabajo 1 (parcial): ComparaciÃ³n YOLO (autosuficiente) ===\n",
        "# Usa YAML_PATH creado en la celda de setup\n",
        "\n",
        "assert 'YAML_PATH' in globals(), 'YAML_PATH no definido. Ejecuta la celda de Setup de Dataset primero.'\n",
        "print('Usando YAML:', YAML_PATH)\n",
        "\n",
        "MODELS = {\n",
        "    'YOLOv8n': 'yolov8n.pt',\n",
        "    'YOLOv8s': 'yolov8s.pt',\n",
        "    'YOLOv11n': 'yolo11n.pt',\n",
        "}\n",
        "\n",
        "EPOCHS = 5\n",
        "BATCH = 16\n",
        "IMG_SIZE = 416\n",
        "FRACTION = 0.25\n",
        "\n",
        "train_summaries = {}\n",
        "val_metrics = {}\n",
        "\n",
        "for name, weights in MODELS.items():\n",
        "    print(f\"\\n=== Entrenando {name} ({weights}) ===\")\n",
        "    model = YOLO(weights)\n",
        "    t0 = time.time()\n",
        "    results = model.train(\n",
        "        data=str(YAML_PATH),\n",
        "        epochs=EPOCHS,\n",
        "        imgsz=IMG_SIZE,\n",
        "        batch=BATCH,\n",
        "        fraction=FRACTION,\n",
        "        device=device_choosen,\n",
        "        name=f\"exp_{name.lower()}\",\n",
        "        project='runs/detect',\n",
        "        verbose=True,\n",
        "        save=True,\n",
        "        plots=True,\n",
        "        patience=3,\n",
        "    )\n",
        "    t1 = time.time()\n",
        "    train_time_min = (t1 - t0) / 60\n",
        "    metrics = model.val()\n",
        "\n",
        "    train_summaries[name] = {\n",
        "        'weights_dir': str(results.save_dir),\n",
        "        'train_time_min': train_time_min,\n",
        "    }\n",
        "    val_metrics[name] = {\n",
        "        'map50': float(metrics.box.map50),\n",
        "        'map5095': float(metrics.box.map),\n",
        "        'precision': float(metrics.box.mp),\n",
        "        'recall': float(metrics.box.mr)\n",
        "    }\n",
        "\n",
        "# Tabla comparativa\n",
        "df = pd.DataFrame(val_metrics).T\n",
        "for k, v in train_summaries.items():\n",
        "    df.loc[k, 'train_time_min'] = v['train_time_min']\n",
        "\n",
        "print(\"\\nğŸ“Š Comparativa rÃ¡pida (subset):\")\n",
        "display(df[['map50','map5095','precision','recall','train_time_min']].sort_values('map50', ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IBJaNadGL8et",
        "outputId": "285a7203-d35b-4297-cba1-eec526b03434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando YAML: fruit_detection/data_fixed.yaml\n",
            "\n",
            "=== Entrenando YOLOv8n (yolov8n.pt) ===\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=fruit_detection/data_fixed.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=0.25, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp_yolov8n2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=3, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/exp_yolov8n2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 21.8MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752482  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,018 parameters, 3,012,002 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 56.8MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1552.6Â±556.8 MB/s, size: 53.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/fruit_detection/Fruits-detection/train/labels... 1777 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1777/1777 2.3Kit/s 0.8s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/fruit_detection/Fruits-detection/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 606.0Â±140.9 MB/s, size: 70.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 1.4Kit/s 0.7s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/fruit_detection/Fruits-detection/valid/labels.cache\n",
            "Plotting labels to /content/runs/detect/exp_yolov8n2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/exp_yolov8n2\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        1/5      1.19G       1.23      3.141      1.298          6        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 4.6it/s 24.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.6it/s 6.2s\n",
            "                   all        914       3227      0.342      0.168      0.103     0.0622\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        2/5       1.5G      1.249       2.23      1.288         19        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.4it/s 20.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.6it/s 5.2s\n",
            "                   all        914       3227      0.229      0.248      0.142      0.082\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        3/5      1.51G      1.145       2.01      1.251          9        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.2it/s 21.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.6it/s 5.2s\n",
            "                   all        914       3227      0.238      0.205      0.139     0.0712\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        4/5      1.52G      1.127      1.794      1.233          3        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.5it/s 20.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.7it/s 5.1s\n",
            "                   all        914       3227      0.325      0.287      0.227      0.127\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        5/5      1.52G      1.028      1.674      1.188          2        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.6it/s 19.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.7it/s 5.1s\n",
            "                   all        914       3227      0.435      0.336      0.287      0.169\n",
            "\n",
            "5 epochs completed in 0.038 hours.\n",
            "Optimizer stripped from /content/runs/detect/exp_yolov8n2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/exp_yolov8n2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/exp_yolov8n2/weights/best.pt...\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.8it/s 7.7s\n",
            "                   all        914       3227      0.434      0.335      0.286      0.169\n",
            "                 Apple        188        557      0.414      0.303       0.26      0.168\n",
            "                Banana        167        390      0.593      0.377      0.387      0.206\n",
            "                 Grape        199        809       0.36      0.269      0.239      0.136\n",
            "                Orange        197       1100      0.299      0.555      0.326      0.192\n",
            "             Pineapple         77        154      0.492      0.239      0.243      0.135\n",
            "            Watermelon        107        217      0.446      0.267      0.264      0.179\n",
            "Speed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/exp_yolov8n2\u001b[0m\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1261.8Â±351.7 MB/s, size: 52.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 1.9Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 58/58 7.1it/s 8.2s\n",
            "                   all        914       3227      0.434      0.336      0.286       0.17\n",
            "                 Apple        188        557      0.412      0.303       0.26      0.168\n",
            "                Banana        167        390      0.596      0.383      0.388      0.206\n",
            "                 Grape        199        809      0.361      0.271       0.24      0.137\n",
            "                Orange        197       1100      0.298      0.555      0.323      0.192\n",
            "             Pineapple         77        154      0.492      0.239      0.242      0.135\n",
            "            Watermelon        107        217      0.447      0.267      0.264       0.18\n",
            "Speed: 0.7ms preprocess, 2.1ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val\u001b[0m\n",
            "\n",
            "=== Entrenando YOLOv8s (yolov8s.pt) ===\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 21.5MB 226.7MB/s 0.1s\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=fruit_detection/data_fixed.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=0.25, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp_yolov8s, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=3, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/exp_yolov8s, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2118370  ultralytics.nn.modules.head.Detect           [6, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,137,922 parameters, 11,137,906 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2213.3Â±595.2 MB/s, size: 63.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/fruit_detection/Fruits-detection/train/labels.cache... 1777 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1777/1777 3.7Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 381.1Â±129.0 MB/s, size: 59.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 1.2Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "Plotting labels to /content/runs/detect/exp_yolov8s/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/exp_yolov8s\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        1/5      1.94G       1.16      2.538      1.258          6        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 4.9it/s 22.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.1it/s 7.0s\n",
            "                   all        914       3227      0.323      0.208      0.173     0.0993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        2/5      2.44G      1.153      1.786       1.23         19        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.5it/s 20.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.4it/s 6.6s\n",
            "                   all        914       3227      0.268      0.289      0.194      0.106\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        3/5      2.46G      1.081       1.63      1.207          9        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.6it/s 20.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.5it/s 6.5s\n",
            "                   all        914       3227      0.245      0.299      0.179      0.101\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        4/5      2.46G      1.046      1.457       1.19          3        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.4it/s 20.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.2it/s 5.6s\n",
            "                   all        914       3227      0.404      0.326      0.285      0.159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        5/5      2.46G     0.9315      1.271      1.141          2        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.4it/s 20.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.1it/s 5.7s\n",
            "                   all        914       3227      0.467      0.383       0.36      0.225\n",
            "\n",
            "5 epochs completed in 0.039 hours.\n",
            "Optimizer stripped from /content/runs/detect/exp_yolov8s/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from /content/runs/detect/exp_yolov8s/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating /content/runs/detect/exp_yolov8s/weights/best.pt...\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 11,127,906 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.7it/s 7.9s\n",
            "                   all        914       3227      0.467      0.383       0.36      0.225\n",
            "                 Apple        188        557      0.491      0.287      0.317      0.223\n",
            "                Banana        167        390      0.542       0.39      0.403      0.216\n",
            "                 Grape        199        809      0.441      0.299       0.29      0.185\n",
            "                Orange        197       1100      0.358      0.552      0.407      0.249\n",
            "             Pineapple         77        154      0.392      0.318      0.261      0.142\n",
            "            Watermelon        107        217       0.58      0.451      0.484      0.335\n",
            "Speed: 0.1ms preprocess, 1.9ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/exp_yolov8s\u001b[0m\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 11,127,906 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1566.0Â±552.6 MB/s, size: 63.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 1.6Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 58/58 6.3it/s 9.2s\n",
            "                   all        914       3227      0.467      0.382       0.36      0.225\n",
            "                 Apple        188        557      0.494      0.289      0.318      0.224\n",
            "                Banana        167        390      0.543       0.39      0.403      0.215\n",
            "                 Grape        199        809      0.441      0.299       0.29      0.186\n",
            "                Orange        197       1100      0.358      0.552      0.407      0.249\n",
            "             Pineapple         77        154      0.394      0.318      0.261      0.142\n",
            "            Watermelon        107        217      0.574      0.447      0.482      0.333\n",
            "Speed: 0.9ms preprocess, 3.8ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val2\u001b[0m\n",
            "\n",
            "=== Entrenando YOLOv11n (yolo11n.pt) ===\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=fruit_detection/data_fixed.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=0.25, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp_yolov11n, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=3, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/exp_yolov11n, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    431842  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
            "YOLO11n summary: 181 layers, 2,591,010 parameters, 2,590,994 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2054.8Â±660.3 MB/s, size: 63.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/fruit_detection/Fruits-detection/train/labels.cache... 1777 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1777/1777 3.7Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 436.0Â±118.0 MB/s, size: 59.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 866.3Kit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "Plotting labels to /content/runs/detect/exp_yolov11n/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/exp_yolov11n\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        1/5      1.44G      1.198      3.281      1.281          6        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 3.0it/s 36.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.5it/s 11.5s\n",
            "                   all        914       3227      0.216      0.122     0.0871     0.0459\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        2/5      1.75G      1.287       2.36      1.316         19        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.0it/s 22.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.6it/s 5.2s\n",
            "                   all        914       3227      0.279      0.245      0.171     0.0897\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        3/5      1.75G      1.175      2.026      1.263          9        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.3it/s 21.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.5it/s 5.3s\n",
            "                   all        914       3227      0.297       0.21      0.167     0.0911\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        4/5      1.75G      1.144       1.83      1.248          3        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.4it/s 20.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 5.0it/s 5.8s\n",
            "                   all        914       3227      0.349      0.292      0.236      0.135\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        5/5      1.75G      1.041      1.677      1.196          2        416: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 5.4it/s 20.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.5it/s 6.4s\n",
            "                   all        914       3227      0.485      0.329       0.33      0.203\n",
            "\n",
            "5 epochs completed in 0.044 hours.\n",
            "Optimizer stripped from /content/runs/detect/exp_yolov11n/weights/last.pt, 5.4MB\n",
            "Optimizer stripped from /content/runs/detect/exp_yolov11n/weights/best.pt, 5.4MB\n",
            "\n",
            "Validating /content/runs/detect/exp_yolov11n/weights/best.pt...\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,583,322 parameters, 0 gradients, 6.3 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.0it/s 7.2s\n",
            "                   all        914       3227      0.489      0.329       0.33      0.203\n",
            "                 Apple        188        557      0.422      0.334      0.314      0.224\n",
            "                Banana        167        390      0.504      0.358      0.381      0.204\n",
            "                 Grape        199        809      0.481      0.274      0.276       0.17\n",
            "                Orange        197       1100      0.454      0.455      0.393      0.227\n",
            "             Pineapple         77        154      0.436      0.247      0.249      0.138\n",
            "            Watermelon        107        217      0.637      0.304      0.369      0.252\n",
            "Speed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/exp_yolov11n\u001b[0m\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,583,322 parameters, 0 gradients, 6.3 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1805.3Â±548.4 MB/s, size: 63.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 1.7Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 58/58 7.8it/s 7.5s\n",
            "                   all        914       3227       0.49      0.328       0.33      0.203\n",
            "                 Apple        188        557      0.422      0.334      0.313      0.224\n",
            "                Banana        167        390      0.507      0.355      0.381      0.205\n",
            "                 Grape        199        809      0.478      0.274      0.276       0.17\n",
            "                Orange        197       1100      0.455      0.455      0.394      0.227\n",
            "             Pineapple         77        154      0.439      0.244       0.25      0.139\n",
            "            Watermelon        107        217      0.637      0.304      0.368      0.252\n",
            "Speed: 0.7ms preprocess, 2.2ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val3\u001b[0m\n",
            "\n",
            "ğŸ“Š Comparativa rÃ¡pida (subset):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             map50   map5095  precision    recall  train_time_min\n",
              "YOLOv8s   0.360200  0.224643   0.467315  0.382479        2.597289\n",
              "YOLOv11n  0.330423  0.203024   0.489563  0.327749        2.933633\n",
              "YOLOv8n   0.286234  0.169612   0.434480  0.336327        2.798310"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c79e83b1-ba0e-4928-a69f-9f7ce26ea1a7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>map50</th>\n",
              "      <th>map5095</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>train_time_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>YOLOv8s</th>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.224643</td>\n",
              "      <td>0.467315</td>\n",
              "      <td>0.382479</td>\n",
              "      <td>2.597289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>YOLOv11n</th>\n",
              "      <td>0.330423</td>\n",
              "      <td>0.203024</td>\n",
              "      <td>0.489563</td>\n",
              "      <td>0.327749</td>\n",
              "      <td>2.933633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>YOLOv8n</th>\n",
              "      <td>0.286234</td>\n",
              "      <td>0.169612</td>\n",
              "      <td>0.434480</td>\n",
              "      <td>0.336327</td>\n",
              "      <td>2.798310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c79e83b1-ba0e-4928-a69f-9f7ce26ea1a7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c79e83b1-ba0e-4928-a69f-9f7ce26ea1a7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c79e83b1-ba0e-4928-a69f-9f7ce26ea1a7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-af06f92e-c04b-4a8e-b4b6-32aa2ec3ffd0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af06f92e-c04b-4a8e-b4b6-32aa2ec3ffd0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-af06f92e-c04b-4a8e-b4b6-32aa2ec3ffd0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df[['map50','map5095','precision','recall','train_time_min']]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"map50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.037216249615881665,\n        \"min\": 0.2862341992586646,\n        \"max\": 0.36020014108298204,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.36020014108298204,\n          0.33042322970055965,\n          0.2862341992586646\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"map5095\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02772541123847066,\n        \"min\": 0.1696115727261684,\n        \"max\": 0.2246427513773626,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2246427513773626,\n          0.20302396773130138,\n          0.1696115727261684\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02771061579469247,\n        \"min\": 0.43448017315730386,\n        \"max\": 0.48956334490551584,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4673149457635133,\n          0.48956334490551584,\n          0.43448017315730386\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029435904717415345,\n        \"min\": 0.3277492026608624,\n        \"max\": 0.3824786029482783,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.3824786029482783,\n          0.3277492026608624,\n          0.336327284755195\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16923804295294653,\n        \"min\": 2.597289025783539,\n        \"max\": 2.933633081118266,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.597289025783539,\n          2.933633081118266,\n          2.7983099937438967\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Visualizaciones y mÃ©tricas comparativas (rÃ¡pidas) ===\n",
        "\n",
        "# Reutiliza df de Trabajo 1 (comparativa modelos)\n",
        "try:\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
        "    # Speed vs Accuracy (mAP50)\n",
        "    ax[0].scatter(df['train_time_min'], df['map50'], s=120, c='coral', edgecolor='k')\n",
        "    for i, name in enumerate(df.index):\n",
        "        ax[0].annotate(name, (df['train_time_min'][i], df['map50'][i]))\n",
        "    ax[0].set_xlabel('Train time (min)')\n",
        "    ax[0].set_ylabel('mAP@0.5')\n",
        "    ax[0].set_title('Speed vs Accuracy (subset)')\n",
        "    ax[0].grid(alpha=0.3)\n",
        "\n",
        "    # Precision vs Recall\n",
        "    ax[1].scatter(df['precision'], df['recall'], s=120, c='steelblue', edgecolor='k')\n",
        "    for i, name in enumerate(df.index):\n",
        "        ax[1].annotate(name, (df['precision'][i], df['recall'][i]))\n",
        "    ax[1].set_xlabel('Precision')\n",
        "    ax[1].set_ylabel('Recall')\n",
        "    ax[1].set_title('Precision vs Recall')\n",
        "    ax[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ No hay df con mÃ©tricas aÃºn. Ejecuta Trabajo 1 primero.\", e)\n",
        "\n",
        "# Placeholder para mÃ©tricas de tracking con motmetrics (si tienes GTs temporales)\n",
        "print(\"\\nâ„¹ï¸ Sugerencia: si cuentas con GTs de tracking (IDs por frame), usa motmetrics para MOTA/MOTP/IDF1.\")\n",
        "print(\"   Este notebook incluye una demo con conteos e ID switches aproximados (no mÃ©tricas MOT completas).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "H4jR0dxhRHn5",
        "outputId": "9c94cc5c-8a8b-42a6-d0d5-e5c376872beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â„¹ï¸ Sugerencia: si cuentas con GTs de tracking (IDs por frame), usa motmetrics para MOTA/MOTP/IDF1.\n",
            "   Este notebook incluye una demo con conteos e ID switches aproximados (no mÃ©tricas MOT completas).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === ComparaciÃ³n extendida: mÃ¡s Ã©pocas y mÃ¡s modelos (YOLOv8/YOLOv11) ===\n",
        "# NOTA: YOLOv5n es opcional (requiere repo/paquete distinto). Dejamos comentario al final.\n",
        "\n",
        "assert 'YAML_PATH' in globals(), 'YAML_PATH no definido. Ejecuta el Setup de Dataset primero.'\n",
        "print('Usando YAML:', YAML_PATH)\n",
        "\n",
        "# Modelos a comparar (Ultralytics >= 8.3, soporta YOLOv8 y YOLOv11)\n",
        "MODELS_EXT = {\n",
        "    'YOLOv8n': 'yolov8n.pt',\n",
        "    'YOLOv8s': 'yolov8s.pt',\n",
        "    'YOLOv8m': 'yolov8m.pt',\n",
        "    'YOLOv11n': 'yolo11n.pt',\n",
        "}\n",
        "\n",
        "# HiperparÃ¡metros mÃ¡s exigentes para ver diferencias\n",
        "EPOCHS_EXT = 10\n",
        "BATCH_EXT = 16\n",
        "IMG_SIZE_EXT = 640\n",
        "FRACTION_EXT = 0.75  # 75% del dataset para acelerar un poco sin perder seÃ±al\n",
        "\n",
        "# RecolecciÃ³n de resultados\n",
        "from time import time\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "extended_rows = []\n",
        "per_model_details = {}\n",
        "\n",
        "val_imgs_dir = None\n",
        "for cand in [\n",
        "    # intentos comunes\n",
        "    Path(DATASET_ROOT) / 'Fruits-detection' / 'valid' / 'images',\n",
        "    Path(DATASET_ROOT) / 'valid' / 'images',\n",
        "    Path(DATASET_ROOT) / 'val' / 'images',\n",
        "]:\n",
        "    if cand.exists():\n",
        "        val_imgs_dir = cand\n",
        "        break\n",
        "\n",
        "# PequeÃ±o helper para tiempo de inferencia\n",
        "def benchmark_inference(yolo_model, sample_paths: list[str], conf: float = 0.25) -> float:\n",
        "    if not sample_paths:\n",
        "        return float('nan')\n",
        "    # warmup\n",
        "    _ = yolo_model(sample_paths[0], conf=conf, verbose=False)\n",
        "    t0 = time()\n",
        "    for p in sample_paths:\n",
        "        _ = yolo_model(p, conf=conf, verbose=False)\n",
        "    t1 = time()\n",
        "    return (t1 - t0) / len(sample_paths) * 1000.0\n",
        "\n",
        "# Seleccionar 20 imÃ¡genes para medir inferencia si es posible\n",
        "sample_paths = []\n",
        "if val_imgs_dir is not None:\n",
        "    cand_imgs = list(val_imgs_dir.glob('*.jpg')) + list(val_imgs_dir.glob('*.png'))\n",
        "    sample_paths = [str(p) for p in cand_imgs[:20]]\n",
        "\n",
        "for name, weights in MODELS_EXT.items():\n",
        "    print(f\"\\n=== Entrenando {name} ({weights}) ===\")\n",
        "    model = YOLO(weights)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    t0 = time()\n",
        "    results = model.train(\n",
        "        data=str(YAML_PATH),\n",
        "        epochs=EPOCHS_EXT,\n",
        "        imgsz=IMG_SIZE_EXT,\n",
        "        batch=BATCH_EXT,\n",
        "        fraction=FRACTION_EXT,\n",
        "        device=device_choosen,\n",
        "        name=f\"exp_ext_{name.lower()}\",\n",
        "        project='runs/detect',\n",
        "        verbose=True,\n",
        "        save=True,\n",
        "        plots=True,\n",
        "        patience=7,\n",
        "    )\n",
        "    t1 = time()\n",
        "    train_time_min = (t1 - t0) / 60\n",
        "\n",
        "    # ValidaciÃ³n\n",
        "    metrics = model.val()\n",
        "\n",
        "    # Tiempos de inferencia\n",
        "    infer_ms = benchmark_inference(model, sample_paths, conf=0.3) if sample_paths else float('nan')\n",
        "\n",
        "    # TamaÃ±o de pesos (best.pt)\n",
        "    weights_dir = Path(results.save_dir) / 'weights'\n",
        "    best_path = weights_dir / 'best.pt'\n",
        "    size_mb = os.path.getsize(best_path) / (1024 * 1024) if best_path.exists() else float('nan')\n",
        "\n",
        "    # GPU memoria mÃ¡xima\n",
        "    gpu_mem_mb = float('nan')\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            gpu_mem_mb = torch.cuda.max_memory_reserved() / (1024 * 1024)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    row = {\n",
        "        'model': name,\n",
        "        'weights_dir': str(results.save_dir),\n",
        "        'map50': float(metrics.box.map50),\n",
        "        'map5095': float(metrics.box.map),\n",
        "        'precision': float(metrics.box.mp),\n",
        "        'recall': float(metrics.box.mr),\n",
        "        'train_time_min': train_time_min,\n",
        "        'infer_ms': infer_ms,\n",
        "        'weights_mb': size_mb,\n",
        "        'gpu_mem_max_mb': gpu_mem_mb,\n",
        "    }\n",
        "    extended_rows.append(row)\n",
        "\n",
        "    # Guardar mÃ©tricas por clase\n",
        "    per_model_details[name] = {\n",
        "        'class_map50': list(metrics.box.maps),\n",
        "        'names': list(model.names.values()),\n",
        "        'best_weights': str(best_path) if best_path.exists() else None,\n",
        "    }\n",
        "\n",
        "# DataFrame comparativo extendido\n",
        "import pandas as pd\n",
        "\n",
        "df_ext = pd.DataFrame(extended_rows).set_index('model').sort_values('map50', ascending=False)\n",
        "print(\"\\nğŸ“Š Comparativa extendida:\")\n",
        "display(df_ext[['map50','map5095','precision','recall','train_time_min','infer_ms','weights_mb','gpu_mem_max_mb']])\n",
        "\n",
        "# Elegir mejor modelo por mAP@0.5\n",
        "best_model_name = df_ext.index[0]\n",
        "print(f\"\\nğŸ¥‡ Mejor modelo por mAP@0.5: {best_model_name}\")\n",
        "\n",
        "# NOTA sobre YOLOv5n (opcional):\n",
        "# Para entrenar YOLOv5n, se sugiere usar el repo oficial (pip install -q yolov5) y su CLI/train.py.\n",
        "# Se deja fuera por simplicidad y para mantener un Ãºnico flujo con Ultralytics v8/11 en este notebook.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3OyF70aV9G6",
        "outputId": "7f173aae-00d0-45b9-82a1-4dd93a038552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando YAML: fruit_detection/data_fixed.yaml\n",
            "\n",
            "=== Entrenando YOLOv8n (yolov8n.pt) ===\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=fruit_detection/data_fixed.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=0.75, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp_ext_yolov8n, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=7, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/exp_ext_yolov8n, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752482  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,018 parameters, 3,012,002 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1268.4Â±373.4 MB/s, size: 53.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/fruit_detection/Fruits-detection/train/labels... 5331 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5331/5331 2.0Kit/s 2.6s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/fruit_detection/Fruits-detection/train/images/3d8be4f881b8c54c_jpg.rf.0d7b6d095459cece040b47b246d807af.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/fruit_detection/Fruits-detection/train/images/3d8be4f881b8c54c_jpg.rf.64e869a9bedd5f012cc2a1129c6ca229.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/fruit_detection/Fruits-detection/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 403.8Â±97.1 MB/s, size: 70.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 364.9Kit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "Plotting labels to /content/runs/detect/exp_ext_yolov8n/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/exp_ext_yolov8n\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/25      3.07G      1.088      2.578      1.306         50        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.4it/s 1:38\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.9it/s 7.4s\n",
            "                   all        914       3227      0.314      0.283      0.206       0.12\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/25      3.73G      1.037      1.888      1.271         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.6it/s 8.1s\n",
            "                   all        914       3227      0.322      0.235      0.172     0.0843\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/25      3.75G      1.042      1.774      1.262         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:29\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.6it/s 8.0s\n",
            "                   all        914       3227      0.347      0.261      0.206      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/25      3.77G       1.02      1.658      1.259          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.7it/s 7.9s\n",
            "                   all        914       3227       0.35      0.251      0.209      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/25      3.77G     0.9896      1.538      1.242         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.5it/s 8.3s\n",
            "                   all        914       3227      0.426      0.293      0.256      0.142\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/25      3.77G      0.962       1.45      1.222         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.0it/s 7.3s\n",
            "                   all        914       3227      0.462      0.318       0.32      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/25      3.77G     0.9462      1.376      1.203         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.4it/s 8.5s\n",
            "                   all        914       3227      0.456      0.359      0.328      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/25      3.77G     0.9228      1.324       1.19         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.9it/s 7.5s\n",
            "                   all        914       3227      0.518      0.372      0.378      0.233\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/25      3.77G     0.8939      1.274      1.184         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.6it/s 8.0s\n",
            "                   all        914       3227      0.504       0.37      0.372      0.224\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/25      3.77G     0.8811      1.212       1.17         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.6it/s 8.1s\n",
            "                   all        914       3227      0.519      0.374      0.386       0.23\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/25      3.77G     0.8542      1.176      1.159         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.2it/s 7.0s\n",
            "                   all        914       3227      0.519      0.385      0.389      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/25      3.77G     0.8477      1.149      1.156          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.5it/s 8.2s\n",
            "                   all        914       3227      0.535      0.385      0.411      0.252\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/25      3.77G      0.839      1.099      1.148          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.2it/s 7.0s\n",
            "                   all        914       3227      0.522      0.422       0.42      0.264\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/25      3.77G     0.8318      1.074      1.138         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:29\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.5it/s 8.4s\n",
            "                   all        914       3227      0.561      0.396      0.416      0.254\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/25      3.77G     0.8147      1.045      1.131         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.7it/s 1:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.0it/s 7.3s\n",
            "                   all        914       3227      0.557      0.418      0.444      0.277\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/25      3.77G       0.83      1.012      1.139          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.8it/s 1:28\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.7it/s 7.8s\n",
            "                   all        914       3227       0.53      0.389      0.403       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/25      3.77G     0.8129     0.9363      1.122          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 4.0it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.5it/s 8.2s\n",
            "                   all        914       3227      0.539      0.416      0.435      0.273\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/25      3.77G     0.7967     0.9098      1.112          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 4.0it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.6it/s 8.0s\n",
            "                   all        914       3227      0.535      0.434      0.446      0.281\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/25      3.78G     0.7839     0.8751      1.108          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 4.0it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.7it/s 7.8s\n",
            "                   all        914       3227      0.606      0.404      0.442      0.285\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/25      3.78G     0.7787     0.8441      1.096         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 4.0it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.7it/s 7.9s\n",
            "                   all        914       3227      0.598      0.409      0.448      0.289\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/25      3.78G      0.762     0.8133      1.094         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 4.0it/s 1:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.8it/s 7.5s\n",
            "                   all        914       3227      0.575      0.413      0.447       0.29\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/25      3.78G     0.7453     0.7787      1.078          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.9it/s 1:25\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.0it/s 7.2s\n",
            "                   all        914       3227      0.576      0.448      0.464      0.296\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/25      3.78G     0.7412     0.7617      1.074         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.9it/s 1:25\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.2it/s 6.9s\n",
            "                   all        914       3227       0.61      0.437      0.479      0.313\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/25      3.78G     0.7325     0.7415       1.07          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.9it/s 1:25\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.2it/s 6.9s\n",
            "                   all        914       3227      0.601      0.442       0.48      0.314\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/25      3.78G     0.7225     0.7131       1.06         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.9it/s 1:25\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 4.2it/s 6.9s\n",
            "                   all        914       3227       0.59      0.439      0.473      0.311\n",
            "\n",
            "25 epochs completed in 0.670 hours.\n",
            "Optimizer stripped from /content/runs/detect/exp_ext_yolov8n/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/exp_ext_yolov8n/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/exp_ext_yolov8n/weights/best.pt...\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.3s\n",
            "                   all        914       3227        0.6      0.443       0.48      0.314\n",
            "                 Apple        188        557      0.624      0.452      0.498      0.346\n",
            "                Banana        167        390      0.663      0.472      0.506      0.297\n",
            "                 Grape        199        809      0.521      0.361      0.375      0.243\n",
            "                Orange        197       1100      0.589      0.367      0.421      0.259\n",
            "             Pineapple         77        154      0.588      0.422      0.471      0.306\n",
            "            Watermelon        107        217      0.617      0.581       0.61      0.432\n",
            "Speed: 0.2ms preprocess, 2.5ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/exp_ext_yolov8n\u001b[0m\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1244.6Â±388.9 MB/s, size: 52.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 1.1Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 58/58 6.1it/s 9.5s\n",
            "                   all        914       3227        0.6      0.443       0.48      0.314\n",
            "                 Apple        188        557      0.621      0.452      0.498      0.346\n",
            "                Banana        167        390       0.66      0.472      0.506      0.297\n",
            "                 Grape        199        809      0.523      0.361      0.375      0.242\n",
            "                Orange        197       1100      0.591      0.369      0.423       0.26\n",
            "             Pineapple         77        154      0.587      0.422      0.471      0.306\n",
            "            Watermelon        107        217      0.617      0.581      0.609      0.432\n",
            "Speed: 1.4ms preprocess, 3.8ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val4\u001b[0m\n",
            "\n",
            "=== Entrenando YOLOv8s (yolov8s.pt) ===\n",
            "Ultralytics 8.3.222 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=fruit_detection/data_fixed.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=0.75, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp_ext_yolov8s, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=7, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/exp_ext_yolov8s, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2118370  ultralytics.nn.modules.head.Detect           [6, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,137,922 parameters, 11,137,906 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1293.7Â±333.2 MB/s, size: 53.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/fruit_detection/Fruits-detection/train/labels.cache... 5331 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5331/5331 8.8Mit/s 0.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/fruit_detection/Fruits-detection/train/images/3d8be4f881b8c54c_jpg.rf.0d7b6d095459cece040b47b246d807af.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/fruit_detection/Fruits-detection/train/images/3d8be4f881b8c54c_jpg.rf.64e869a9bedd5f012cc2a1129c6ca229.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 465.5Â±68.3 MB/s, size: 70.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 196.8Kit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n",
            "Plotting labels to /content/runs/detect/exp_ext_yolov8s/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/exp_ext_yolov8s\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/25      4.33G      1.025       2.12      1.257         50        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.1it/s 1:47\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.2it/s 9.0s\n",
            "                   all        914       3227      0.344      0.332       0.26      0.145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/25      5.48G      1.006      1.588      1.258         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.2it/s 1:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.4it/s 8.5s\n",
            "                   all        914       3227      0.371      0.294      0.243      0.133\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/25      5.48G      1.014      1.584      1.259         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.9s\n",
            "                   all        914       3227      0.364      0.244       0.21      0.113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/25      5.48G     0.9737      1.488      1.239          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.2it/s 8.9s\n",
            "                   all        914       3227      0.379      0.329      0.271      0.158\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/25      5.48G     0.9396      1.385      1.221         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.8s\n",
            "                   all        914       3227      0.507      0.369      0.355       0.21\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/25      5.52G     0.9019      1.297      1.193         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.9s\n",
            "                   all        914       3227      0.451      0.353      0.348      0.207\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/25      5.55G     0.8867      1.233      1.178         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.8s\n",
            "                   all        914       3227      0.493      0.367       0.36       0.22\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/25      5.59G     0.8653      1.167       1.17         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.5it/s 8.2s\n",
            "                   all        914       3227      0.545      0.387      0.408      0.253\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/25      5.62G     0.8297      1.107      1.156         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:41\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.2it/s 9.1s\n",
            "                   all        914       3227      0.527      0.391      0.407      0.255\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/25      5.65G     0.8183      1.058      1.141         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.6it/s 8.0s\n",
            "                   all        914       3227      0.552      0.411      0.429      0.266\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/25      5.65G     0.7869      1.017      1.129         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.9s\n",
            "                   all        914       3227      0.525      0.404      0.409      0.253\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/25      5.65G     0.7835     0.9996      1.128          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:43\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.5it/s 8.2s\n",
            "                   all        914       3227      0.509      0.448      0.436      0.276\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/25      5.65G     0.7688      0.945      1.117          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.9s\n",
            "                   all        914       3227      0.517      0.424      0.427      0.267\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/25      5.65G     0.7647     0.9192      1.107         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.4it/s 8.5s\n",
            "                   all        914       3227      0.602      0.425      0.468      0.297\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/25      5.65G     0.7466     0.8933      1.097         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 334/334 3.3it/s 1:42\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.7s\n",
            "                   all        914       3227      0.634       0.42      0.475      0.308\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/25      5.65G     0.7652     0.8573      1.108         34        640: 54% â”â”â”â”â”â”â”€â”€â”€â”€â”€â”€ 179/334 3.5it/s 54.7s<44.8s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === VisualizaciÃ³n: detecciones con boxes en imÃ¡genes de validaciÃ³n ===\n",
        "\n",
        "assert 'per_model_details' in globals() and 'df_ext' in globals(), 'Ejecuta la celda de ComparaciÃ³n extendida primero.'\n",
        "\n",
        "# Tomar top-2 modelos por mAP@0.5 para comparar visualmente\n",
        "top_models = list(df_ext.index[:2])\n",
        "print('Modelos para visualizaciÃ³n:', top_models)\n",
        "\n",
        "# Construir modelos cargando sus best.pt\n",
        "loaded = {}\n",
        "for m in top_models:\n",
        "    best_w = per_model_details[m]['best_weights']\n",
        "    if best_w and Path(best_w).exists():\n",
        "        loaded[m] = YOLO(best_w)\n",
        "    else:\n",
        "        # fallback a pesos base si no hay best\n",
        "        print(f\"âš ï¸ No se encontrÃ³ best.pt para {m}, usando pesos base pre-entrenados\")\n",
        "        loaded[m] = YOLO(MODELS_EXT[m])\n",
        "\n",
        "# Elegir 3 imÃ¡genes de validaciÃ³n\n",
        "if val_imgs_dir is None:\n",
        "    raise RuntimeError('No se encontraron imÃ¡genes de validaciÃ³n para visualizar.')\n",
        "\n",
        "val_images_list = list(val_imgs_dir.glob('*.jpg')) + list(val_imgs_dir.glob('*.png'))\n",
        "num_samples = min(3, len(val_images_list))\n",
        "assert num_samples > 0, 'No hay imÃ¡genes de validaciÃ³n disponibles.'\n",
        "\n",
        "sample_images = random.sample(val_images_list, num_samples)\n",
        "\n",
        "for i, img_path in enumerate(sample_images, 1):\n",
        "    print(f\"\\n=== Imagen {i}/{num_samples}: {img_path.name} ===\")\n",
        "    fig, axes = plt.subplots(1, len(top_models), figsize=(7*len(top_models), 7))\n",
        "    if len(top_models) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, (mname, mobj) in zip(axes, loaded.items()):\n",
        "        res = mobj(str(img_path), conf=0.30, verbose=False)\n",
        "        ann = res[0].plot()\n",
        "        ax.imshow(cv2.cvtColor(ann, cv2.COLOR_BGR2RGB))\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"{mname}\\n{len(res[0].boxes)} detecciones\", fontsize=13, fontweight='bold')\n",
        "\n",
        "    plt.suptitle(f'ComparaciÃ³n visual en validaciÃ³n: {img_path.name}', fontsize=15, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print('\\nâœ… VisualizaciÃ³n completada')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "v5ViPBhftgUF",
        "outputId": "10f6fb3d-1ce2-49be-ef41-73790790a812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Ejecuta la celda de ComparaciÃ³n extendida primero.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3606402364.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# === VisualizaciÃ³n: detecciones con boxes en imÃ¡genes de validaciÃ³n ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'per_model_details'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'df_ext'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ejecuta la celda de ComparaciÃ³n extendida primero.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tomar top-2 modelos por mAP@0.5 para comparar visualmente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Ejecuta la celda de ComparaciÃ³n extendida primero."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === GrÃ¡ficos: Speed vs Accuracy y mAP por clase ===\n",
        "\n",
        "assert 'df_ext' in globals() and 'per_model_details' in globals(), 'Ejecuta la celda de ComparaciÃ³n extendida primero.'\n",
        "\n",
        "# 1) Speed vs Accuracy (mAP@0.5 vs infer_ms)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(df_ext['infer_ms'], df_ext['map50'], s=140, c='coral', edgecolor='k')\n",
        "for name in df_ext.index:\n",
        "    plt.annotate(name, (df_ext.loc[name, 'infer_ms'], df_ext.loc[name, 'map50']))\n",
        "plt.xlabel('Inference time (ms/img)')\n",
        "plt.ylabel('mAP@0.5')\n",
        "plt.title('Speed vs Accuracy (valid)')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 2) mAP por clase para el mejor modelo\n",
        "best_name = df_ext.index[0]\n",
        "class_map = per_model_details[best_name]['class_map50']\n",
        "class_names = per_model_details[best_name]['names']\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(class_map)), class_map, color='steelblue', alpha=0.85)\n",
        "plt.xticks(range(len(class_map)), class_names, rotation=30, ha='right')\n",
        "plt.ylabel('mAP@0.5')\n",
        "plt.title(f'mAP por clase - {best_name}')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nâ„¹ï¸ Los grÃ¡ficos de losses por modelo estÃ¡n en runs/detect/<exp>/results.png (Ultralytics).')\n",
        "\n"
      ],
      "metadata": {
        "id": "lhV-MSJYtr35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}