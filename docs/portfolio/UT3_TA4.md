---
title: "Plantilla de entrada de portafolio"
date: 2025-01-01
---

# Plantilla de entrada de portafolio

## Contexto

En esta actividad se aplica el modelo Segment Anything Model (SAM) para realizar segmentaci√≥n sem√°ntica en la detecci√≥n de zonas inundadas en im√°genes satelitales. SAM se utiliza primero en modo zero-shot, mostrando sus capacidades generales, pero tambi√©n sus limitaciones en dominios espec√≠ficos como flood-mapping. Luego se realiza fine-tuning utilizando un dataset rotulado de √°reas inundadas para mejorar la precisi√≥n de las m√°scaras.

El objetivo es comparar el desempe√±o entre el modelo base y el modelo ajustado, analizando m√©tricas, visualizaciones y casos de error, validando la utilidad del fine-tuning para aplicaciones cr√≠ticas como monitoreo de desastres naturales y respuesta en tiempo real.

## Objetivos

Evaluar el desempe√±o del modelo SAM para segmentaci√≥n de inundaciones y demostrar las mejoras logradas mediante fine-tuning en un dataset espec√≠fico, comparando resultados cuantitativos y cualitativos.

- Ejecutar inferencia zero-shot con SAM pretrained
- Implementar points/boxes y evaluar resultados
- Fine-tuning del decoder de SAM con dataset de inundaciones
- Comparar m√©tricas entre pretrained y fine-tuned
- Visualizar ejemplos antes vs despu√©s

## Actividades

- Parte 1: Load Dataset y Exploraci√≥n
    - 1.1 Download Flood Area Segmentation Dataset
        - Paso 1: Obtener API Key de Kaggle
        - Paso 2: Configurar Kaggle API en Colab/Jupyter
        - Paso 3: Descargar y Descomprimir Dataset
    - 1.2 Explorar Dataset
- Parte 2: Pretrained SAM Inference
    - 2.1 Cargar SAM Model
    - 2.2 Crear SAM Predictor
    - 2.3 Inference con Point Prompts
    - 2.4 Inference con Box Prompts
    - 2.5 Calcular M√©tricas
    - 2.6 Evaluaci√≥n en Test Set Completo
- Parte 3: Fine-tuning SAM
    - 3.1 Crear Dataset Class
    - 3.2 Setup DataLoader
    - 3.3 Definir Loss Function
    - 3.4 Fine-tuning Setup
    - 3.5 Training Loop
    - 3.6 Training
- Parte 4: Evaluaci√≥n y Comparaci√≥n
    - 4.1 Cargar Best Model
    - 4.2 Comparaci√≥n Pretrained vs Fine-tuned
    - 4.3 Visualizaci√≥n Cualitativa
    - 4.4 An√°lisis de Errores

## Desarrollo

### Exploraci√≥n del dataset

Se implement√≥ un cargador personalizado para leer pares imagen-m√°scara, validando que cada imagen tuviera su correspondiente m√°scara. Se obtuvieron 100 muestras iniciales para an√°lisis preliminar. Las im√°genes fueron convertidas a RGB y las m√°scaras binarizadas para asegurar consistencia en el entrenamiento y evaluaci√≥n.

Se realiz√≥ un an√°lisis exploratorio que evidenci√≥ una alta variabilidad en las dimensiones de las im√°genes (81 tama√±os √∫nicos), lo cual refuerza la necesidad de aplicar transformaciones de normalizaci√≥n m√°s adelante durante el preprocesamiento. Adem√°s, se calcul√≥ la proporci√≥n promedio de p√≠xeles correspondientes a agua, obteniendo un water ratio aproximado de 42.80%, lo que indica un dataset relativamente balanceado entre clases agua y fondo.

```python
üì• Cargando 100 im√°genes...
  Cargadas 20/100...
  Cargadas 40/100...
  Cargadas 60/100...
  Cargadas 80/100...
  Cargadas 100/100...
‚úÖ Cargadas 100 im√°genes con sus m√°scaras
=== DATASET CARGADO ===
Total images: 100
Image shape (primera imagen): (551, 893, 3)
Mask shape (primera m√°scara): (551, 893)

üìä Estad√≠sticas del dataset:
Tama√±os √∫nicos de im√°genes: 81

Water pixel ratio (promedio): 42.80%
Background ratio: 57.20%
```

Finalmente, se visualizaron muestras del dataset, confirmando visualmente la correcta alineaci√≥n entre im√°genes y m√°scaras, y verificando la calidad de las anotaciones. Esta etapa permiti√≥ validar que los datos estaban listos para ser procesados por SAM tanto en modo pretrained como para fine-tuning.

![](../assets/UT3_TA4_1.png)

### Evaluaci√≥n del modelo SAM preentrenado

Tras preparar y explorar el dataset, se procedi√≥ a integrar el modelo SAM en modalidad zero-shot para establecer una l√≠nea base de rendimiento antes del fine-tuning.

En esta etapa se definieron dos estrategias de inferencia:

1. Point Prompt: se seleccion√≥ autom√°ticamente un p√≠xel representativo dentro del √°rea inundada de la m√°scara real, simulando que un analista marca la regi√≥n de inter√©s.

![](../assets/UT3_TA4_2.png)


2. Box Prompt: se gener√≥ un bounding box a partir de la m√°scara real, emulando un escenario donde un detector preclasifica zonas potenciales de inundaci√≥n.

![](../assets/UT3_TA4_3.png)

Se implementaron funciones espec√≠ficas para realizar predicciones con ambos tipos de entrada, eligiendo la m√°scara con mayor confianza entre m√∫ltiples hip√≥tesis del modelo. Adem√°s, se programaron m√©tricas cuantitativas est√°ndar para segmentaci√≥n (IoU, Dice, Precisi√≥n y Recall), asegurando el ajuste autom√°tico de dimensiones y formatos de m√°scaras.

```python
=== M√âTRICAS - POINT PROMPT ===
IoU: 0.8070
Dice: 0.8932
Precision: 0.9681
Recall: 0.8290

=== M√âTRICAS - BOX PROMPT ===
IoU: 0.8016
Dice: 0.8899
Precision: 0.9756
Recall: 0.8180

=== COMPARACI√ìN ===
Box prompt better: False
```

Inicialmente se ejecutaron pruebas visuales sobre una sola imagen, confirmando que SAM identific√≥ correctamente las √°reas de agua, con alta confianza en ambos enfoques. Posteriormente, se evaluaron las 100 im√°genes del subconjunto, calculando m√©tricas promedio y distribuciones.

```python
=== EVALUATING PRETRAINED SAM (Point Prompts) ===
  Processed 20/100 images...
  Processed 40/100 images...
  Processed 60/100 images...
  Processed 80/100 images...
  Processed 100/100 images...

=== PRETRAINED SAM - POINT PROMPTS ===
Mean IoU: 0.5291 ¬± 0.3214
Mean Dice: 0.6220 ¬± 0.3377
Mean Precision: 0.8193
Mean Recall: 0.5885

=== EVALUATING PRETRAINED SAM (Box Prompts) ===
  Processed 20/100 images...
  Processed 40/100 images...
  Processed 60/100 images...
  Processed 80/100 images...
  Processed 100/100 images...

=== PRETRAINED SAM - BOX PROMPTS ===
Mean IoU: 0.7230 ¬± 0.2088
Mean Dice: 0.8156 ¬± 0.1985
Mean Precision: 0.8476
Mean Recall: 0.8106
```

![](../assets/UT3_TA4_4.png)

### Fine-tuning de SAM para segmentaci√≥n de inundaciones

Luego del establecimiento del baseline zero-shot con SAM, se procedi√≥ al fine-tuning del modelo. Esta etapa tuvo como objetivo evaluar si la adaptaci√≥n, incluso con un dataset relativamente peque√±o (100 im√°genes), pod√≠a mejorar el rendimiento obtenido.

Se construy√≥ un dataset personalizado en PyTorch que:

- Redimensiona im√°genes y m√°scaras a 1024√ó1024.
- Implementa data augmentation (flip horizontal/vertical, rotaci√≥n ligera, variaci√≥n de brillo/contraste).
- Extrae autom√°ticamente prompts basados en la m√°scara real.
- Soporta de se√±ale, point prompt con una coordenada aleatoria dentro del agua.

Divisi√≥n de datos:

```python
=== DATA SPLIT ===
Train: 80 images
Val: 20 images

=== DATALOADERS CREADOS ===
Train batches: 40
Val batches: 10

Sample batch:
  Images shape: torch.Size([2, 3, 1024, 1024])
  Masks shape: torch.Size([2, 1, 1024, 1024])
  Prompts: 2 items
```

Se adopt√≥ una estrategia eficiente entrenando solo el cabezal de segmentaci√≥n:

| M√≥dulo SAM     | Estado       |
| -------------- | ------------ |
| Image encoder  | ‚ùÑÔ∏è Congelado |
| Prompt encoder | ‚ùÑÔ∏è Congelado |
| Mask decoder   | ‚úÖ Entrenable |

Esto reduce riesgo de overfitting y acelera entrenamiento:

- Par√°metros totales: 93.7M
- Par√°metros entrenables: 4.06M (‚âà4.3%)

```python
=== FINE-TUNING SETUP ===
Total parameters: 93,735,472
Trainable parameters: 4,058,340
Trainable %: 4.33%

Optimizer: Adam
Learning rate: 0.0001
Scheduler: StepLR (decay every 5 epochs by 0.5)
```

```python
=== STARTING TRAINING ===
Epochs: 10
Batch size: 2
Learning rate: 0.0001

Epoch 1/10
--------------------------------------------------
Train Loss: 0.4536 | Train IoU: 0.5152
Val Loss: 0.4255 | Val IoU: 0.6116
‚úÖ Best model saved! (Val IoU: 0.6116)

Epoch 2/10
--------------------------------------------------
Train Loss: 0.3343 | Train IoU: 0.6362
Val Loss: 0.3314 | Val IoU: 0.6984
‚úÖ Best model saved! (Val IoU: 0.6984)

Epoch 3/10
--------------------------------------------------
Train Loss: 0.3207 | Train IoU: 0.6622
Val Loss: 0.4353 | Val IoU: 0.5396

Epoch 4/10
--------------------------------------------------
Train Loss: 0.3284 | Train IoU: 0.6684
Val Loss: 0.4232 | Val IoU: 0.6481

Epoch 5/10
--------------------------------------------------
Train Loss: 0.2892 | Train IoU: 0.6932
Val Loss: 0.3636 | Val IoU: 0.6449

Epoch 6/10
--------------------------------------------------
Train Loss: 0.2819 | Train IoU: 0.6946
Val Loss: 0.3170 | Val IoU: 0.7110
‚úÖ Best model saved! (Val IoU: 0.7110)

Epoch 7/10
--------------------------------------------------
Train Loss: 0.2804 | Train IoU: 0.6936
Val Loss: 0.3144 | Val IoU: 0.6994

Epoch 8/10
--------------------------------------------------
Train Loss: 0.2478 | Train IoU: 0.7255
Val Loss: 0.3415 | Val IoU: 0.6934

Epoch 9/10
--------------------------------------------------
Train Loss: 0.2495 | Train IoU: 0.7154
Val Loss: 0.2717 | Val IoU: 0.7575
‚úÖ Best model saved! (Val IoU: 0.7575)

Epoch 10/10
--------------------------------------------------
Train Loss: 0.2464 | Train IoU: 0.7266
Val Loss: 0.3023 | Val IoU: 0.7237

=== TRAINING COMPLETED ===
Best Val IoU: 0.7575
```

Best Val IoU: 0.7575

Respecto al baseline zero-shot point prompt (~0.52 IoU), el fine-tuning implic√≥ una mejora de aproximadamente de +0.24 IoU

La curva mostr√≥:

- Reducci√≥n consistente de loss
- Incremento sostenido de IoU
- Ligera oscilaci√≥n en validaci√≥n t√≠pica por el tama√±o del dataset
- No evidencias fuertes de overfitting

![](../assets/UT3_TA4_5.png)

### Evaluaci√≥n del modelo

Luego de completar el entrenamiento, se carg√≥ el checkpoint del modelo fine-tuned y se realiz√≥ la evaluaci√≥n sobre el conjunto de validaci√≥n. El objetivo fue comparar su desempe√±o frente al modelo SAM pre-entrenado, utilizando m√©tricas (IoU, Dice, precisi√≥n y recall).

En t√©rminos generales, el modelo ajustado mostr√≥ mejoras claras en todas las m√©tricas evaluadas. Los mayores avances se observaron especialmente en IoU y Dice, indicando que el fine-tuning permiti√≥ que el modelo se adapte mejor a las caracter√≠sticas espec√≠ficas del dataset trabajado. Esto se traduce en segmentos m√°s fieles a las regiones reales en las im√°genes.

```python
=== EVALUATING FINE-TUNED SAM ===
  Processed 20/20 images...

=== FINE-TUNED SAM ===
Mean IoU: 0.7180 ¬± 0.2132
Mean Dice: 0.8126 ¬± 0.1902
Mean Precision: 0.8893
Mean Recall: 0.7707

=== COMPARISON ===
Metric          Pretrained      Fine-tuned      Improvement    
------------------------------------------------------------
IOU             0.5291          0.7180          35.69          %
DICE            0.6220          0.8126          30.65          %
PRECISION       0.8193          0.8893          8.55           %
RECALL          0.5885          0.7707          30.96          %
```

![](../assets/UT3_TA4_6.png)

Al analizar los resultados caso por caso, se identificaron mejoras particularmente notorias en im√°genes donde el modelo pre-entrenado ten√≠a dificultades para identificar correctamente las regiones objetivo. El fine-tuned logr√≥ corregir segmentaciones incompletas y reducir errores en zonas m√°s delgadas o menos contrastadas. Si bien hubo ejemplos donde la mejora fue m√≠nima (principalmente en im√°genes que ya eran f√°ciles para el modelo original), los beneficios en los casos complejos justifican el proceso de ajuste.

```python
=== IMAGE 0 ===
Pretrained: IoU=0.0018, Dice=0.0036
Fine-tuned: IoU=0.6744, Dice=0.8056
Improvement: IoU +0.6726, Dice +0.8019
```

![](../assets/UT3_TA4_7.png)

```python
=== IMAGE 5 ===
Pretrained: IoU=0.5862, Dice=0.7391
Fine-tuned: IoU=0.8350, Dice=0.9101
Improvement: IoU +0.2488, Dice +0.1709
```

![](../assets/UT3_TA4_8.png)

```python
=== IMAGE 10 ===
Pretrained: IoU=0.9338, Dice=0.9658
Fine-tuned: IoU=0.9348, Dice=0.9663
Improvement: IoU +0.0010, Dice +0.0005
```

![](../assets/UT3_TA4_9.png)

```python
=== IMAGE 15 ===
Pretrained: IoU=0.2934, Dice=0.4537
Fine-tuned: IoU=0.3018, Dice=0.4636
Improvement: IoU +0.0084, Dice +0.0100
```

![](../assets/UT3_TA4_10.png)


Finalmente, tambi√©n se revisaron fallos. El modelo ajustado no solo redujo el n√∫mero total de errores, sino que adem√°s las fallas restantes presentaron menor desviaci√≥n respecto al ground truth. Esto confirma que el fine-tuning no solo mejora la precisi√≥n global sino tambi√©n la estabilidad del modelo frente a distintos tipos de im√°genes.


```python
=== ANALYZING PRETRAINED FAILURES ===
Failure cases: 7

Failure statistics:
  Mean IoU: 0.092
  Mean water region width: 1.00 pixels
```

![](../assets/UT3_TA4_11.png)

```python
=== ANALYZING FINE-TUNED FAILURES ===
Failure cases: 2

=== FAILURE REDUCTION ===
Pretrained failures: 7
Fine-tuned failures: 2
Reduction: 5 (71.4%)
```

![](../assets/UT3_TA4_12.png)

## Reflexi√≥n

Esta practica me permiti√≥ recorrer todo el flujo para adaptar y evaluar SAM en el dominio de segmentaci√≥n de inundaciones, desde la preparaci√≥n del dataset hasta la comparaci√≥n entre inferencia zero-shot y fine-tuning. Se analizo el desempe√±o del modelo preentrenado, y el hecho de entender las ventajas y limitaciones de distintos tipos de prompts, y dise√±ar una estrategia de entrenamiento eficiente para un dataset reducido.

Uno de los principales aprendizajes fue comprobar que, si bien SAM ofrece resultados fuertes en modo zero-shot, la especializaci√≥n resulta clave para maximizar la precisi√≥n en contextos espec√≠ficos como im√°genes satelitales de inundaciones. El fine-tuning logr√≥ mejoras significativas, redujo casos fallidos y gener√≥ predicciones m√°s completas y consistentes.
