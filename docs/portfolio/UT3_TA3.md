---
title: "YOLOv8 Fine-tuning & Tracking"
date: 2025-01-01
---

# YOLO Fine-tuning & Tracking

## Contexto

Un equipo de Computer Vision de una cadena de supermercados necesita mejorar la detecci√≥n y el seguimiento de productos en entornos de grocery. El modelo YOLOv8 pre-entrenado en COCO falla al identificar productos espec√≠ficos, por lo que se propone fine-tuning sobre un dataset especializado y a√±adir un m√≥dulo de tracking para aplicaciones reales, control de inventario en estantes, conteo en cintas de checkout, y monitoreo de reposici√≥n en tiempo real. 

## Objetivos

Mejorar la detecci√≥n y el seguimiento de productos en entornos de supermercado mediante el uso de un modelo YOLOv8 fine-tuned sobre un conjunto de datos espec√≠fico de productos, permitiendo una identificaci√≥n m√°s precisa y un seguimiento continuo en video.

- Implementar la inferencia con un modelo YOLOv8 preentrenado para establecer una l√≠nea base de rendimiento.
- Realizar el fine-tuning del modelo utilizando un dataset de productos de grocery, con el fin de adaptar el detector a las caracter√≠sticas visuales reales del entorno.
- Evaluar las mejoras obtenidas despu√©s del entrenamiento mediante m√©tricas est√°ndar y an√°lisis cualitativo de los resultados.
- Analizar los errores m√°s comunes del modelo (falsos positivos y negativos) para comprender sus limitaciones y posibles mejoras futuras.
- Implementar un sistema de tracking que permita seguir los productos detectados a lo largo del tiempo en videos, facilitando tareas de monitoreo, conteo y control de inventario.

## Actividades

- Parte 1: Setup e Inferencia B√°sica
    - Paso 1.1: Instalaci√≥n
    - Paso 1.2: Cargar Modelo Base
    - Paso 1.3: Test en Im√°genes de Grocery
- Parte 2: Fine-tuning YOLOv8 en Fruit Detection Dataset
    - Paso 2.1: Descargar Dataset de Frutas (YOLOv8 Format)
    - Paso 2.1b: Verificar Estructura y data.yaml
    - Paso 2.2: Explorar Dataset
    - Paso 2.3: Visualizar Ejemplos del Dataset
    - Paso 2.4: Arreglar data.yaml y Fine-tuning YOLOv8
    - Paso 2.5: Cargar Modelo Fine-tuned
    - Paso 2.6: M√©tricas de Evaluaci√≥n
    - Paso 2.7: Comparaci√≥n Antes vs Despu√©s
    - Paso 2.8: An√°lisis de Errores
- Parte 3: Tracking con Modelo Fine-tuned
    - Paso 3.1: Descargar Video de Frutas
    - Paso 3.2: Configurar Norfair Tracker
    - Paso 3.3: Aplicar Tracking en Video
    - Paso 3.4: Visualizar Video con Tracking
    - Paso 3.5: An√°lisis de Tracking

## Desarrollo

### Parte 1: Setup e Inferencia B√°sica

Como modelo base se seleccion√≥ YOLOv8n (nano), la versi√≥n m√°s liviana de la familia YOLOv8. Modelos m√°s grandes (como yolov8m o yolov8l) ofrecen mayor precisi√≥n, pero requieren mayor capacidad de c√≥mputo, lo cual no es necesario en esta fase inicial de an√°lisis.

El modelo cargado fue preentrenado en el dataset COCO, que contiene 80 clases gen√©ricas. Sin embargo, estas clases son demasiado generales para nuestro caso de uso, ya que el objetivo es detectar productos espec√≠ficos de supermercado. Aunque COCO incluye categor√≠as como apple o orange, su entrenamiento se basa en ejemplos gen√©ricos y no representa adecuadamente las condiciones visuales reales del entorno grocery.

Una vez cargado el modelo, se realiz√≥ una prueba de inferencia sobre una imagen realista de supermercado. Se utiliz√≥ un umbral de confianza de 0.2, que permite detectar objetos sin exigir alta certeza inicial, para observar el comportamiento general del modelo.

![](../assets/UT3_TA3_1.png)

El modelo logr√≥ detectar cinco objetos, identificados como oranges y broccolis. Se evidenci√≥ que no es capaz de reconocer correctamente productos espec√≠ficos.

El experimento confirma que el modelo YOLOv8 preentrenado en COCO no resulta adecuado para tareas espec√≠ficas de detecci√≥n de productos de supermercado. Esto justifica la necesidad de un proceso de fine-tuning con un dataset especializado, que contenga im√°genes representativas del entorno real y clases espec√≠ficas del dominio de grocery.

### Parte 2: Preparaci√≥n del Dataset y Entrenamiento del Modelo

Una vez configurado el entorno, se procedi√≥ a descargar el conjunto de datos necesario para el entrenamiento del modelo. Para este proyecto se utiliz√≥ el Fruit Detection Dataset. Este conjunto contiene im√°genes de distintas frutas, ya etiquetadas y listas para ser usadas en tareas de detecci√≥n de objetos.

Posteriormente, se realiz√≥ un an√°lisis exploratorio del conjunto de datos. Se revis√≥ la cantidad de im√°genes y etiquetas disponibles, y se evalu√≥ la distribuci√≥n de las clases. Este an√°lisis permiti√≥ observar que algunas clases ten√≠an muchas m√°s im√°genes que otras, lo que puede influir en el rendimiento del modelo.

```python
üìä Estad√≠sticas:
Total de im√°genes: 8479

=== DISTRIBUCI√ìN DE CLASES (TRAIN) ===
Total de clases: 6
Clases del dataset: ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']

Apple               : 6070 instancias
Banana              : 2971 instancias
Grape               : 6027 instancias
Orange              : 13938 instancias
Pineapple           : 1372 instancias
Watermelon          : 1683 instancias

üìä ESTAD√çSTICAS ADICIONALES:
  Instancias totales: 32061
  Promedio por clase: 5343.5
  Clase m√°s frecuente: Orange (13938 instancias)
  Clase menos frecuente: Pineapple (1372 instancias)
```

![](../assets/UT3_TA3_2.png) 

Para complementar el an√°lisis, se visualizaron algunas im√°genes del conjunto de entrenamiento junto con las cajas delimitadoras que indican la posici√≥n de cada fruta. Esto permiti√≥ confirmar que las anotaciones estaban bien hechas y que el modelo recibir√≠a datos de calidad.

![](../assets/UT3_TA3_3.png) 

Finalmente, se configuraron los par√°metros b√°sicos del proceso de fine-tuning, incluyendo la cantidad de √©pocas, el tama√±o de las im√°genes y el tama√±o del lote de datos procesado en cada iteraci√≥n. Se utiliz√≥ un modelo base preentrenado YOLOv8n, que se adapt√≥ a las nuevas clases del dataset de frutas.

```python
Model summary: 129 layers, 11,137,922 parameters, 11,137,906 gradients, 28.7 GFLOPs

optimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... 
optimizer: AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)
Image sizes 640 train, 640 val
Starting training for 30 epochs...

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/30      8.45G      1.207      3.041      1.312        162        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56/56 1.5it/s 36.9s
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/15 1.5it/s 9.7s
                   all        914       3227      0.263      0.228      0.156     0.0866

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/30      7.58G     0.9088      1.137      1.177        121        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56/56 1.8it/s 30.9s
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/15 1.6it/s 9.4s
                   all        914       3227      0.415      0.336      0.307      0.174

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      20/30      7.37G     0.7606     0.7735      1.077        150        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56/56 1.8it/s 31.1s
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/15 1.5it/s 10.0s
                   all        914       3227      0.518      0.383      0.382      0.233
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      30/30      7.26G     0.6313     0.4888     0.9906         86        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56/56 1.8it/s 31.4s
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/15 1.7it/s 9.0s
                   all        914       3227      0.578      0.392      0.418       0.27

30 epochs completed in 0.353 hours.

Model summary (fused): 72 layers, 11,127,906 parameters, 0 gradients, 28.4 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/15 1.3it/s 11.9s
                   all        914       3227      0.583       0.39      0.418       0.27
                 Apple        188        557      0.591      0.354      0.379      0.266
                Banana        167        390      0.564      0.444       0.44      0.253
                 Grape        199        809      0.564      0.347      0.363      0.229
                Orange        197       1100      0.653      0.355      0.405      0.256
             Pineapple         77        154      0.583      0.354      0.413       0.25
            Watermelon        107        217      0.545      0.488       0.51      0.364
Speed: 0.2ms preprocess, 5.0ms inference, 0.0ms loss, 2.8ms postprocess per image
```

![](../assets/UT3_TA3_4.png) 

El entrenamiento se desarroll√≥ correctamente, mostrando en cada √©poca el progreso del modelo en t√©rminos de precisi√≥n y detecci√≥n. A medida que avanzaba el proceso, se observ√≥ una mejora en la capacidad del modelo para reconocer las diferentes frutas.

Luego se carg√≥ el modelo resultante del proceso de entrenamiento, seleccionando el correspondiente a los mejores pesos. El modelo cargado fue un YOLOv8 fine-tuned, especializado en la detecci√≥n de frutas. Mientras que el modelo base (entrenado en el conjunto COCO) reconoc√≠a 80 clases gen√©ricas, el modelo ajustado se centr√≥ en 6 clases espec√≠ficas: manzana, banana, uva, naranja, anan√° y sand√≠a. Esta especializaci√≥n permiti√≥ reducir la confusi√≥n con objetos no relacionados y mejorar la precisi√≥n en el dominio espec√≠fico.

Una vez cargado el modelo, se evalu√≥ su desempe√±o sobre el conjunto de validaci√≥n. Los resultados mostraron valores moderados de precisi√≥n y recall, indicando que el modelo fue capaz de reconocer correctamente varias frutas, aunque a√∫n exist√≠an casos de detecciones faltantes o incorrectas.

Entre las clases, sand√≠a obtuvo el mejor desempe√±o individual, mientras que uva y banana mostraron mayores dificultades. Esto se explica en parte por la distribuci√≥n desigual de las clases en el dataset y por las similitudes visuales entre ciertos tipos de frutas.

En general, el modelo fine-tuned mostr√≥ una mejora clara respecto al modelo base, al enfocarse exclusivamente en frutas y evitar confusiones con objetos gen√©ricos del dataset COCO.

```python
=== EVALUACI√ìN EN VALIDATION SET ===
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 58/58 4.1it/s 14.1s
                   all        914       3227      0.578      0.392      0.418       0.27
                 Apple        188        557      0.582      0.354       0.38      0.267
                Banana        167        390      0.557      0.444       0.44      0.253
                 Grape        199        809      0.562       0.35      0.363       0.23
                Orange        197       1100      0.653      0.357      0.404      0.256
             Pineapple         77        154       0.58      0.357      0.411       0.25
            Watermelon        107        217      0.536      0.488       0.51      0.364

üìä M√âTRICAS DEL MODELO FINE-TUNED:
  mAP@0.5:     0.418
  mAP@0.5:0.95: 0.270
  Precision:   0.578
  Recall:      0.392

=== M√âTRICAS POR CLASE ===
Apple               : mAP@0.5 = 0.267
Banana              : mAP@0.5 = 0.253
Grape               : mAP@0.5 = 0.230
Orange              : mAP@0.5 = 0.256
Pineapple           : mAP@0.5 = 0.250
Watermelon          : mAP@0.5 = 0.364
```

Para visualizar las mejoras, se realiz√≥ una comparaci√≥n directa entre ambos modelos sobre un mismo conjunto de im√°genes del set de validaci√≥n.

En las pruebas, se observ√≥ que el modelo base detectaba m√∫ltiples objetos irrelevantes (como personas o plantas), mientras que el modelo fine-tuned identificaba correctamente frutas concretas en la escena. Sin embargo, tambi√©n se detectaron algunos casos donde el modelo ajustado no reconoci√≥ objetos que estaban presentes, lo que evidencia margen para optimizaci√≥n adicional.

En conjunto, la comparaci√≥n visual mostr√≥ que el modelo fine-tuned logra detecciones m√°s precisas y con cajas delimitadoras mejor ajustadas, adem√°s de una mayor coherencia en las clases detectadas.

![](../assets/UT3_TA3_5.png) 

![](../assets/UT3_TA3_6.png) 

![](../assets/UT3_TA3_7.png) 

Los resultados indicaron que el modelo base comet√≠a m√°s errores de falsa detecci√≥n, mientras que el modelo fine-tuned redujo significativamente estos casos, aumentando la precisi√≥n general. A pesar de que a√∫n se observaron falsos negativos, el balance global fue positivo.

En t√©rminos cualitativos, el modelo ajustado mostr√≥ un incremento en precisi√≥n y en la capacidad de generalizaci√≥n dentro del dominio de frutas.

```python 
=== AN√ÅLISIS DE ERRORES ===

=== RESULTADOS COMPARATIVOS ===

Modelo Base (COCO):
  TP: 0, FP: 5, FN: 6
  Precision: 0.000
  Recall:    0.000
  F1-Score:  0.000

Modelo Fine-tuned:
  TP: 3, FP: 0, FN: 3
  Precision: 1.000
  Recall:    0.500
  F1-Score:  0.667

=== MEJORA ===
  Œî Precision: +1.000
  Œî Recall:    +0.500
  Œî F1-Score:  +0.667
```

![](../assets/UT3_TA3_8.png) 

El fine-tuning permiti√≥ adaptar un modelo gen√©rico a un dominio espec√≠fico, obteniendo mejoras notables en la detecci√≥n de frutas. Si bien los valores de mAP y recall todav√≠a pueden optimizarse, el modelo fine-tuned representa un avance significativo respecto al modelo base, demostrando que la personalizaci√≥n del entrenamiento es efectiva cuando se dispone de un dataset representativo del contexto de aplicaci√≥n.

### Parte 3: Tracking con Modelo Fine-tuned

En esta etapa se aplic√≥ el modelo fine-tuned previamente entrenado para realizar el seguimiento de productos en un video. El prop√≥sito fue simular un escenario real, como una cinta transportadora o el monitoreo de estantes, donde las frutas se desplazan dentro del campo visual de la c√°mara.

Se utiliz√≥ un video que muestra frutas en movimiento. El video sirvi√≥ como base para aplicar el modelo de detecci√≥n y analizar el comportamiento del sistema de tracking.

Para el seguimiento de objetos se emple√≥ la librer√≠a Norfair. Se configuraron los par√°metros principales del tracker, tales como el umbral de distancia para asociar detecciones entre frames, la cantidad de frames que un objeto puede permanecer sin ser detectado, y el retraso necesario para confirmar la aparici√≥n de un nuevo objeto.

Esta configuraci√≥n permite que el sistema mantenga un equilibrio entre precisi√≥n y estabilidad, evitando que los objetos pierdan su identificaci√≥n ante movimientos r√°pidos o detecciones inconsistentes.

El modelo fine-tuned se aplic√≥ sobre cada cuadro del video, detectando las frutas presentes y asoci√°ndolas a sus identificadores de track. Cada objeto fue representado mediante un recuadro de color y un identificador √∫nico (ID) que se mantuvo a lo largo del tiempo. De esta forma, fue posible seguir a cada fruta mientras se desplazaba, incluso cuando sal√≠a y volv√≠a al campo de visi√≥n de la c√°mara.

```python
üé¨ Procesando video con tracking...
   Modelo: Fine-tuned
   Esto puede tomar 1-2 minutos...
     Procesados 30/343 frames (8.7%)
     Procesados 60/343 frames (17.5%)
     Procesados 90/343 frames (26.2%)
     Procesados 120/343 frames (35.0%)
     Procesados 150/343 frames (43.7%)
     Procesados 180/343 frames (52.5%)
     Procesados 210/343 frames (61.2%)
     Procesados 240/343 frames (70.0%)
     Procesados 270/343 frames (78.7%)
     Procesados 300/343 frames (87.5%)
     Procesados 330/343 frames (96.2%)

‚úÖ Video tracking completado!
   Output guardado: videos/grocery_tracked.mp4
   Total tracks creados: 6
```

El sistema logr√≥ rastrear varios productos a lo largo del video, manteniendo la coherencia de los identificadores en la mayor√≠a de los casos. Algunas frutas, como las naranjas, presentaron un seguimiento m√°s estable, mientras que otras, como las bananas, tuvieron tracks m√°s breves.

```python
======================================================================
ESTAD√çSTICAS DE TRACKING
======================================================================

üìä Estad√≠sticas generales:
  Total productos trackeados: 6
  Duraci√≥n promedio: 98.8 frames (3.3s)
  Duraci√≥n m√°xima: 191 frames (6.4s)
  Duraci√≥n m√≠nima: 6 frames (0.2s)

üìã Detalle por producto trackeado:
Track ID     Clase                Duraci√≥n        Rango Frames        
----------------------------------------------------------------------
Track 1      Orange                187 frames ( 6.2s)    2 ‚Üí 188 
Track 2      Orange                191 frames ( 6.4s)   45 ‚Üí 235 
Track 3      Banana                186 frames ( 6.2s)  129 ‚Üí 314 
Track 4      Banana                  8 frames ( 0.3s)  186 ‚Üí 193 
Track 5      Orange                  6 frames ( 0.2s)  319 ‚Üí 324 
Track 6      Orange                 15 frames ( 0.5s)  328 ‚Üí 342 

üì¶ Productos por clase:
  Orange              :   4 tracks
  Banana              :   2 tracks


‚ö° M√©tricas de calidad del tracking:
  Tracks cortos (<1s):  3 (50.0%)
  Tracks largos (>3s):  3 (50.0%)
  Tracks totales:       6
```

![](../assets/UT3_TA3_9.png)

El experimento demostr√≥ que el modelo fine-tuned, combinado con el tracker de Norfair, puede realizar un seguimiento eficaz de productos en movimiento. Sin embargo, para lograr un rendimiento m√°s robusto en entornos reales, ser√≠a necesario optimizar los par√°metros del tracker y mejorar la consistencia de las detecciones del modelo.

## Reflexi√≥n

A lo largo del desarrollo se logr√≥ comprender el proceso de detecci√≥n, fine-tuning y seguimiento de objetos mediante modelos de visi√≥n. El trabajo permiti√≥ no solo aplicar t√©cnicas pr√°cticas, sino tambi√©n analizar los resultados y entender las limitaciones de cada componente del sistema.

### Sobre el Modelo

La mejora m√°s significativa del fine-tuning se observ√≥ en la reducci√≥n de falsos positivos y falsos negativos, as√≠ como en un aumento del mAP, lo que demuestra que el modelo logr√≥ adaptarse correctamente al dominio espec√≠fico de los productos del supermercado.

El modelo base preentrenado en COCO no fue in√∫til, al contrario, sirvi√≥ como una excelente base generalista, ya que pose√≠a conocimiento previo sobre formas y patrones similares. Sin embargo, su precisi√≥n inicial era insuficiente para las clases concretas del dataset.

Esta experiencia ense√±√≥ que para adaptar un modelo a un nuevo dominio es fundamental partir de un modelo preentrenado y luego ajustar las √∫ltimas capas con ejemplos espec√≠ficos y bien anotados.

### Sobre los Datos

El dataset de 8.479 im√°genes result√≥ adecuado, aunque no excesivo. El hecho de que con solo un 25% de las im√°genes ya se obtuvieran buenos resultados se explica por la coherencia visual del conjunto y la calidad de las etiquetas.

La calidad de las anotaciones fue clave, se observ√≥ que los errores o imprecisiones en los bounding boxes afectaban directamente la estabilidad del entrenamiento y la precisi√≥n final del modelo.

Si se agregaran 1.000 im√°genes m√°s, deber√≠an enfocarse en casos dif√≠ciles, frutas parcialmente ocluidas, variaciones de iluminaci√≥n, distintos √°ngulos y fondos m√°s complejos, para mejorar la robustez del modelo.

### Sobre el Tracking

En el proceso de seguimiento, tanto el modelo como los par√°metros del tracker resultaron importantes, pero la configuraci√≥n del tracker fue decisiva para mantener la consistencia de los IDs y evitar saltos de identidad.

Norfair fue suficiente para esta practica.

El sistema podr√≠a fallar en escenarios con oclusiones prolongadas, iluminaci√≥n variable o movimientos abruptos, donde las detecciones pierden coherencia entre frames.

### Sobre el Deployment

El sistema podr√≠a ejecutarse casi en tiempo real dependiendo del hardware; ser√≠a necesario mantener al menos 25‚Äì30 FPS para un monitoreo fluido.

En casos extremos, como oclusiones o variaciones de luz, podr√≠an integrarse modelos complementarios o mecanismos de correcci√≥n temporal para mantener la estabilidad del seguimiento.

### Trade-offs y Decisiones

Durante el proyecto se identificaron varios trade-offs importantes:

1. Velocidad vs Precisi√≥n: aumentar la resoluci√≥n y el tama√±o del modelo mejora la detecci√≥n, pero incrementa el tiempo de inferencia.
2. Cantidad de epochs vs Riesgo de overfitting: entrenar m√°s tiempo mejora el mAP en el dataset de entrenamiento, pero puede degradar el rendimiento en datos nuevos.
3. Umbral de confianza vs Cobertura: un umbral alto reduce falsos positivos, pero puede eliminar detecciones v√°lidas.

## Referencias

- https://colab.research.google.com/drive/1tuo4GpSfLMyMH1pruYZrBCWCp5Gh2Iqr?usp=sharing
